from bs4 import BeautifulSoup
import requests
import os
from datetime import datetime, timedelta
from config import BASE_URL, HEADERS
from utils import extract_article_data, scrape_article_content, human_delay, classify_category
from supabase import create_client, Client

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# Initialize the Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def article_exists(article_url):
    """
    Checks if an article with the given URL already exists in the Supabase database.
    Replaces the old SQLite check.
    """
    try:
        # Query the 'articles' table for any row where the 'url' matches our article_url
        response = supabase.table('articles').select('id').eq('url', article_url).execute()
        
        # If any data is returned, the article exists
        return len(response.data) > 0
        
    except Exception as e:
        print(f"[!] Error checking for existing article: {e}")
        # If there's an error, assume it exists to avoid duplicates
        return True

def insert_article(article_meta):
    """
    Inserts a new article into the Supabase 'articles' table.
    Replaces the old SQLite insert.
    """
    # Map your scraped data to the column names in your Supabase table
    article_data = {
        "title": article_meta['title'],
        "excerpt": article_meta['excerpt'],
        "publish_time": article_meta['publish_time'], # Ensure this is a datetime object or ISO string
        "url": article_meta['url'],
        "content": article_meta['content'],
        "category": article_meta['category'],
        # 'id' and 'scraped_at' are auto-generated by Supabase, so we don't need to include them
    }
    
    try:
        response = supabase.table('articles').insert(article_data).execute()
        print(f"[+] Successfully inserted article into Supabase: {article_meta['title']}")
        
    except Exception as e:
        print(f"[!] Failed to insert article into Supabase: {e}")

def delete_old_articles():
    """
    (Optional) Deletes articles older than a certain date to keep the database size manageable.
    This is an example: deletes articles scraped more than 30 days ago.
    """
    try:
        # Calculate the date 1 day ago
        cutoff_date = (datetime.now() - timedelta(days=1)).isoformat()
        
        # Delete articles where 'scraped_at' is older than the cutoff date
        response = supabase.table('articles').lt('scraped_at', cutoff_date).delete().execute()
        
        if response.data:
            print(f"[+] Deleted {len(response.data)} old article(s) from Supabase.")
            
    except Exception as e:
        print(f"[!] Error deleting old articles: {e}")

# --- The rest of your original script remains unchanged ---

def get_latest_news_page():
    response = requests.get(BASE_URL, headers=HEADERS, timeout=10)
    response.raise_for_status()
    return BeautifulSoup(response.text, 'html.parser')

def scrape_once():

    if not check_supabase_connection():
        print("[!] Scraping aborted due to database connection failure.")
        return
    
    print(f"\n[âœ“] Checking for new articles at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    try:
        soup = get_latest_news_page()
    except Exception as e:
        print(f"[!] Failed to fetch main page: {e}")
        return

    article_blocks = soup.find_all('article', class_='story')
    new_count = 0

    for article in article_blocks:
        meta = extract_article_data(article)
        if not meta['url']:
            continue

        if article_exists(meta['url']):
            print(f"[=] Article already in database: {meta['title']}")
            continue

        print(f"\n[â†’] Scraping new article: {meta['title']}")
        print(f"URL: {meta['url']}")
        human_delay()

        content = scrape_article_content(meta['url'])
        meta['content'] = content

        # ðŸ”¥ Classify article category (use title + excerpt + content)
        text_for_classification = f"{meta['title']} {meta['excerpt']}"
        meta['category'] = classify_category(text_for_classification)

        insert_article(meta) # This now calls the Supabase function
        new_count += 1

    if new_count == 0:
        print("[=] No new articles found.")
    else:
        print(f"[+] {new_count} new article(s) added to DB.")


# Initialize the Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- NEW: DATABASE CONNECTION CHECK FUNCTION ---
def check_supabase_connection():
    """
    Attempts a simple query to verify the connection to Supabase is working.
    """
    print("[ ] Attempting to connect to Supabase database...")
    try:
        # Try a simple, low-cost query (e.g., get the count of records)
        response = supabase.table('articles').select("id", count="exact").limit(1).execute()
        # If no exception is raised, the connection is successful
        print("[âœ“] Successfully connected to Supabase database.")
        print(f"[i] Found {response.count} total articles in the database.")
        return True
    except Exception as e:
        # This will catch connection errors, invalid credentials, etc.
        print(f"[!] Failed to connect to Supabase database: {e}")
        print(f"[!] Please check your SUPABASE_URL and SUPABASE_KEY in config_supabase.py")
        return False


if __name__ == "__main__":
    scrape_once()