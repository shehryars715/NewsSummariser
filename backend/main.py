from scrape import scrape_once, check_supabase_connection, delete_old_articles
from faiss_store import faiss_create
from rich.console import Console
from rich.panel import Panel
from rich import print
import time
import sys

# Initialize Rich console for pretty output
console = Console()

def run_scraping_cycle():
    
    console.print(Panel.fit("üì∞ Article Scraper to Supabase", style="bold blue"))
    console.print("Starting the scraping process...\n")

    # 1. Check Database Connection First
    console.rule("Step 1: Database Connection Check")
    if not check_supabase_connection():
        console.print("\n[red]‚ùå Aborting. Could not establish a database connection.[/red]")
        sys.exit(1)  # Exit with an error code
    console.print("[green]‚úÖ Database connection confirmed.[/green]\n")

    # 2. Run the Scraper
    console.rule("Step 2: Scraping Articles")
    start_time = time.time()  # Start a timer

    try:
        scrape_once()  # Main scraping function
    except KeyboardInterrupt:
        console.print("\n[yellow]‚ö†Ô∏è  Process interrupted by user. Exiting gracefully.[/yellow]")
        sys.exit(0)
    except Exception as e:
        console.print(f"\n[red]‚ùå A critical error occurred during scraping: {e}[/red]")
        import traceback
        console.print(f"[red]{traceback.format_exc()}[/red]")
    finally:
        elapsed_time = time.time() - start_time
        console.print(f"\n[i] Total execution time: {elapsed_time:.2f} seconds[/i]")

    # 3. Delete Old Articles
    console.rule("Step 3: Deleting Old Articles")
    try:
        delete_old_articles()
    except Exception as e:
        console.print(f"[red]‚ùå Failed to delete old articles: {e}[/red]")

    # 4. Conclusion
    console.rule("Cycle Complete")
    console.print("[green]‚úÖ Scraping cycle finished.[/green]\n")


if __name__ == "__main__":
    console.print("[bold blue]Starting continuous scraping every 2 hours...[/bold blue]\n")
    while True:
        run_scraping_cycle()
        faiss_create()  # Update FAISS index after scraping
        console.print("[i]Waiting 2 hours before next cycle...[/i]\n")
        time.sleep(7200)  # Sleep for 2 hours (7200 seconds)
